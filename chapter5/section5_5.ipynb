{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        },
        "colab_type": "code",
        "id": "HeC1svlM-R7I",
        "outputId": "2855f391-754c-44ef-8ef4-6defe071edc3"
      },
      "outputs": [],
      "source": [
        "# #colabを使う方はこちらを使用ください。\n",
        "# !pip install torch==1.5.0\n",
        "# !pip install torchvision==0.6.0\n",
        "# !pip install torchtext==0.3.1\n",
        "# !pip install numpy==1.21.6\n",
        "# !pip install matplotlib==3.2.2\n",
        "# !pip install Pillow==7.1.2\n",
        "# !pip install opencv-python==4.6.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "n1UMMbciAeIm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "#torchtextを使用\n",
        "from torchtext import data\n",
        "from torchtext import vocab\n",
        "from torchtext import datasets\n",
        "\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "M5NOEqmsAeSH",
        "outputId": "bd690bf7-b6de-453e-94a2-91cdb43386e0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 2,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# データとモデルに.to(device)を指定してgpuの計算資源を使用する。\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "14EADkKAQc6U"
      },
      "source": [
        "#文章生成"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "soKa-e9tQfJ1"
      },
      "source": [
        "## データの読み込み"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "colab_type": "code",
        "id": "tatmAOVUAeVl",
        "outputId": "d3b42adc-8a87-479d-d111-bf1e812e6592"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "downloading ptb.train.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ptb.train.txt: 5.10MB [00:00, 52.1MB/s]                   \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "downloading ptb.valid.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ptb.valid.txt: 400kB [00:00, 10.5MB/s]                   \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "downloading ptb.test.txt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ptb.test.txt: 450kB [00:00, 11.6MB/s]                   \n",
            ".vector_cache/glove.6B.zip: 862MB [01:18, 11.0MB/s]                           \n",
            "100%|█████████▉| 399783/400000 [00:50<00:00, 8062.25it/s]"
          ]
        }
      ],
      "source": [
        "tokenize = lambda x: x.split()\n",
        "# 前処理用の機能のFieldをセットアップ\n",
        "#Field\n",
        "TEXT = data.Field(sequential=True, tokenize=tokenize, lower=True, batch_first=True)\n",
        "\n",
        "# データを取得\n",
        "# The Penn Treebankデータセット。\n",
        "train_dataset, val_dataset, test_dataset = datasets.PennTreebank.splits(TEXT)\n",
        "\n",
        "TEXT.build_vocab(train_dataset, vectors=vocab.GloVe(name='6B', dim=300))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "colab_type": "code",
        "id": "WR8xqHw8GajP",
        "outputId": "998de980-e2ef-4b8c-ecf7-1cfe9c2ecc55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10001\n",
            "[('the', 50770), ('<unk>', 45020), ('<eos>', 42068), ('n', 32481), ('of', 24400), ('to', 23638), ('a', 21196), ('in', 18000), ('and', 17474), (\"'s\", 9784)]\n",
            "['<unk>', '<pad>', 'the', '<eos>', 'n', 'of', 'to', 'a', 'in', 'and']\n"
          ]
        }
      ],
      "source": [
        "#全単語数\n",
        "vocab_size = len(TEXT.vocab)\n",
        "print(vocab_size)\n",
        "# 単語の件数のtop10\n",
        "print(TEXT.vocab.freqs.most_common(10))\n",
        "# 単語\n",
        "print(TEXT.vocab.itos[:10])\n",
        "\n",
        "#埋め込みベクトルを取得\n",
        "word_embeddings = TEXT.vocab.vectors\n",
        "# ハイパーパラメータ\n",
        "embedding_length = 300\n",
        "hidden_size = 256\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "colab_type": "code",
        "id": "P3RbA_s1Ah3O",
        "outputId": "14c258db-1f7e-47f9-a1ff-91d27432861d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "969\n",
            "77\n",
            "86\n"
          ]
        }
      ],
      "source": [
        "# BPTTIteratorは言語モデル用のイテレータ作成を行います。\n",
        "# textとtarget属性を持ちます。\n",
        "train_iter, val_iter, test_iter = data.BPTTIterator.splits(\n",
        "    (train_dataset, val_dataset, test_dataset),\n",
        "    batch_size=32,\n",
        "    bptt_len=30,\n",
        "    repeat=False\n",
        ")\n",
        "\n",
        "print(len(train_iter))\n",
        "print(len(val_iter))\n",
        "print(len(test_iter))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "colab_type": "code",
        "id": "nImqWKVbAh6d",
        "outputId": "69f1711d-1e2f-4293-a107-4acd9fdf6791"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "データの形状確認\n",
            "torch.Size([30, 32])\n",
            "torch.Size([30, 32])\n",
            "permuteでバッチを先にする\n",
            "torch.Size([32, 30])\n",
            "torch.Size([32, 30])\n",
            "データ目の形状とデータを確認\n",
            "torch.Size([30])\n",
            "torch.Size([30])\n",
            "[38, 34, 853, 7536, 1315, 6, 591, 19, 2, 236, 5, 40, 124, 3, 125, 2, 209, 591, 34, 937, 55, 383, 12, 216, 4, 247, 72, 1024, 3, 216]\n",
            "[34, 853, 7536, 1315, 6, 591, 19, 2, 236, 5, 40, 124, 3, 125, 2, 209, 591, 34, 937, 55, 383, 12, 216, 4, 247, 72, 1024, 3, 216, 383]\n",
            "データの単語列を表示\n",
            "['company', 'will', 'begin', 'mailing', 'materials', 'to', 'shareholders', 'at', 'the', 'end', 'of', 'this', 'week', '<eos>', 'under', 'the', 'offer', 'shareholders', 'will', 'receive', 'one', 'right', 'for', 'each', 'n', 'common', 'shares', 'owned', '<eos>', 'each']\n",
            "['will', 'begin', 'mailing', 'materials', 'to', 'shareholders', 'at', 'the', 'end', 'of', 'this', 'week', '<eos>', 'under', 'the', 'offer', 'shareholders', 'will', 'receive', 'one', 'right', 'for', 'each', 'n', 'common', 'shares', 'owned', '<eos>', 'each', 'right']\n"
          ]
        }
      ],
      "source": [
        "for i, batch in enumerate(train_iter):\n",
        "    print('データの形状確認')\n",
        "    print(batch.text.size())\n",
        "    print(batch.target.size())\n",
        "    #BPTTIteratorがBatch firstになってない件は2018/11/24時点では#462がPull requestsがされています。\n",
        "    print('permuteでバッチを先にする')\n",
        "    print(batch.text.permute(1, 0).size())\n",
        "    print(batch.target.permute(1, 0).size())\n",
        "    print('データ目の形状とデータを確認')\n",
        "    text = batch.text.permute(1, 0)\n",
        "    target = batch.target.permute(1, 0)\n",
        "    print(text[1,:].size())\n",
        "    print(target[1,:].size())\n",
        "    print(text[1,:].tolist())\n",
        "    print(target[1,:].tolist())\n",
        "    print('データの単語列を表示')\n",
        "    print([TEXT.vocab.itos[data] for data in  text[1,:].tolist()])\n",
        "    print([TEXT.vocab.itos[data] for data in  target[1,:].tolist()])\n",
        "            \n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7F6l4OjQQlFC"
      },
      "source": [
        "## ネットワークを定義"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "XMj7HAguLNqS"
      },
      "outputs": [],
      "source": [
        "class LstmLangModel(nn.Module):\n",
        "    def __init__(self, batch_size, hidden_size, vocab_size, embedding_length, weights):\n",
        "        super(LstmLangModel, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed = nn.Embedding(vocab_size, embedding_length)\n",
        "        self.embed.weight.data.copy_(weights)\n",
        "        self.lstm = nn.LSTM(embedding_length, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "    \n",
        "    def forward(self, x, h):\n",
        "        x = self.embed(x)\n",
        "        output_seq, (h, c) = self.lstm(x, h)\n",
        "        # 出力を変形する (batch_size*sequence_length, 隠れ層のユニット数hidden_size)\n",
        "        out = output_seq.reshape(output_seq.size(0)*output_seq.size(1), output_seq.size(2))\n",
        "        out = self.fc(out) \n",
        "        return out, (h, c)\n",
        "\n",
        "net = LstmLangModel(batch_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "net = net.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Ht_4zbkmNZF7"
      },
      "outputs": [],
      "source": [
        "# 損失関数、最適化関数を定義\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optim = optim.Adam(filter(lambda p: p.requires_grad, net.parameters()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-XD9DCP8QnhC"
      },
      "source": [
        "## 学習"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3417
        },
        "colab_type": "code",
        "id": "8l2u4A-YNcri",
        "outputId": "fe81469f-294e-4783-9e4e-cb5c3b2e0b27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/200], Loss: 5.8131, Perplexity: 334.66\n",
            "Epoch [2/200], Loss: 5.0673, Perplexity: 158.74\n",
            "Epoch [3/200], Loss: 4.7584, Perplexity: 116.56\n",
            "Epoch [4/200], Loss: 4.5410, Perplexity: 93.79\n",
            "Epoch [5/200], Loss: 4.3679, Perplexity: 78.88\n",
            "Epoch [6/200], Loss: 4.2214, Perplexity: 68.13\n",
            "Epoch [7/200], Loss: 4.0926, Perplexity: 59.89\n",
            "Epoch [8/200], Loss: 3.9768, Perplexity: 53.35\n",
            "Epoch [9/200], Loss: 3.8713, Perplexity: 48.01\n",
            "Epoch [10/200], Loss: 3.7737, Perplexity: 43.54\n",
            "Epoch [11/200], Loss: 3.6819, Perplexity: 39.72\n",
            "Epoch [12/200], Loss: 3.5955, Perplexity: 36.43\n",
            "Epoch [13/200], Loss: 3.5150, Perplexity: 33.62\n",
            "Epoch [14/200], Loss: 3.4382, Perplexity: 31.13\n",
            "Epoch [15/200], Loss: 3.3643, Perplexity: 28.91\n",
            "Epoch [16/200], Loss: 3.2940, Perplexity: 26.95\n",
            "Epoch [17/200], Loss: 3.2267, Perplexity: 25.20\n",
            "Epoch [18/200], Loss: 3.1625, Perplexity: 23.63\n",
            "Epoch [19/200], Loss: 3.1020, Perplexity: 22.24\n",
            "Epoch [20/200], Loss: 3.0453, Perplexity: 21.02\n",
            "Epoch [21/200], Loss: 2.9920, Perplexity: 19.93\n",
            "Epoch [22/200], Loss: 2.9418, Perplexity: 18.95\n",
            "Epoch [23/200], Loss: 2.8905, Perplexity: 18.00\n",
            "Epoch [24/200], Loss: 2.8428, Perplexity: 17.16\n",
            "Epoch [25/200], Loss: 2.7976, Perplexity: 16.41\n",
            "Epoch [26/200], Loss: 2.7546, Perplexity: 15.71\n",
            "Epoch [27/200], Loss: 2.7108, Perplexity: 15.04\n",
            "Epoch [28/200], Loss: 2.6688, Perplexity: 14.42\n",
            "Epoch [29/200], Loss: 2.6283, Perplexity: 13.85\n",
            "Epoch [30/200], Loss: 2.5911, Perplexity: 13.34\n",
            "Epoch [31/200], Loss: 2.5565, Perplexity: 12.89\n",
            "Epoch [32/200], Loss: 2.5239, Perplexity: 12.48\n",
            "Epoch [33/200], Loss: 2.4925, Perplexity: 12.09\n",
            "Epoch [34/200], Loss: 2.4614, Perplexity: 11.72\n",
            "Epoch [35/200], Loss: 2.4309, Perplexity: 11.37\n",
            "Epoch [36/200], Loss: 2.4027, Perplexity: 11.05\n",
            "Epoch [37/200], Loss: 2.3733, Perplexity: 10.73\n",
            "Epoch [38/200], Loss: 2.3474, Perplexity: 10.46\n",
            "Epoch [39/200], Loss: 2.3223, Perplexity: 10.20\n",
            "Epoch [40/200], Loss: 2.2976, Perplexity:  9.95\n",
            "Epoch [41/200], Loss: 2.2741, Perplexity:  9.72\n",
            "Epoch [42/200], Loss: 2.2501, Perplexity:  9.49\n",
            "Epoch [43/200], Loss: 2.2264, Perplexity:  9.27\n",
            "Epoch [44/200], Loss: 2.2045, Perplexity:  9.07\n",
            "Epoch [45/200], Loss: 2.1818, Perplexity:  8.86\n",
            "Epoch [46/200], Loss: 2.1593, Perplexity:  8.67\n",
            "Epoch [47/200], Loss: 2.1402, Perplexity:  8.50\n",
            "Epoch [48/200], Loss: 2.1210, Perplexity:  8.34\n",
            "Epoch [49/200], Loss: 2.1008, Perplexity:  8.17\n",
            "Epoch [50/200], Loss: 2.0829, Perplexity:  8.03\n",
            "Epoch [51/200], Loss: 2.0634, Perplexity:  7.87\n",
            "Epoch [52/200], Loss: 2.0452, Perplexity:  7.73\n",
            "Epoch [53/200], Loss: 2.0285, Perplexity:  7.60\n",
            "Epoch [54/200], Loss: 2.0134, Perplexity:  7.49\n",
            "Epoch [55/200], Loss: 1.9978, Perplexity:  7.37\n",
            "Epoch [56/200], Loss: 1.9851, Perplexity:  7.28\n",
            "Epoch [57/200], Loss: 1.9712, Perplexity:  7.18\n",
            "Epoch [58/200], Loss: 1.9564, Perplexity:  7.07\n",
            "Epoch [59/200], Loss: 1.9429, Perplexity:  6.98\n",
            "Epoch [60/200], Loss: 1.9291, Perplexity:  6.88\n",
            "Epoch [61/200], Loss: 1.9162, Perplexity:  6.80\n",
            "Epoch [62/200], Loss: 1.9060, Perplexity:  6.73\n",
            "Epoch [63/200], Loss: 1.8935, Perplexity:  6.64\n",
            "Epoch [64/200], Loss: 1.8822, Perplexity:  6.57\n",
            "Epoch [65/200], Loss: 1.8723, Perplexity:  6.50\n",
            "Epoch [66/200], Loss: 1.8606, Perplexity:  6.43\n",
            "Epoch [67/200], Loss: 1.8507, Perplexity:  6.36\n",
            "Epoch [68/200], Loss: 1.8402, Perplexity:  6.30\n",
            "Epoch [69/200], Loss: 1.8313, Perplexity:  6.24\n",
            "Epoch [70/200], Loss: 1.8235, Perplexity:  6.19\n",
            "Epoch [71/200], Loss: 1.8138, Perplexity:  6.13\n",
            "Epoch [72/200], Loss: 1.8040, Perplexity:  6.07\n",
            "Epoch [73/200], Loss: 1.7976, Perplexity:  6.03\n",
            "Epoch [74/200], Loss: 1.7862, Perplexity:  5.97\n",
            "Epoch [75/200], Loss: 1.7797, Perplexity:  5.93\n",
            "Epoch [76/200], Loss: 1.7751, Perplexity:  5.90\n",
            "Epoch [77/200], Loss: 1.7653, Perplexity:  5.84\n",
            "Epoch [78/200], Loss: 1.7566, Perplexity:  5.79\n",
            "Epoch [79/200], Loss: 1.7486, Perplexity:  5.75\n",
            "Epoch [80/200], Loss: 1.7422, Perplexity:  5.71\n",
            "Epoch [81/200], Loss: 1.7372, Perplexity:  5.68\n",
            "Epoch [82/200], Loss: 1.7308, Perplexity:  5.65\n",
            "Epoch [83/200], Loss: 1.7231, Perplexity:  5.60\n",
            "Epoch [84/200], Loss: 1.7153, Perplexity:  5.56\n",
            "Epoch [85/200], Loss: 1.7108, Perplexity:  5.53\n",
            "Epoch [86/200], Loss: 1.7041, Perplexity:  5.50\n",
            "Epoch [87/200], Loss: 1.6977, Perplexity:  5.46\n",
            "Epoch [88/200], Loss: 1.6911, Perplexity:  5.43\n",
            "Epoch [89/200], Loss: 1.6854, Perplexity:  5.39\n",
            "Epoch [90/200], Loss: 1.6788, Perplexity:  5.36\n",
            "Epoch [91/200], Loss: 1.6760, Perplexity:  5.34\n",
            "Epoch [92/200], Loss: 1.6706, Perplexity:  5.32\n",
            "Epoch [93/200], Loss: 1.6625, Perplexity:  5.27\n",
            "Epoch [94/200], Loss: 1.6604, Perplexity:  5.26\n",
            "Epoch [95/200], Loss: 1.6527, Perplexity:  5.22\n",
            "Epoch [96/200], Loss: 1.6472, Perplexity:  5.19\n",
            "Epoch [97/200], Loss: 1.6412, Perplexity:  5.16\n",
            "Epoch [98/200], Loss: 1.6367, Perplexity:  5.14\n",
            "Epoch [99/200], Loss: 1.6356, Perplexity:  5.13\n",
            "Epoch [100/200], Loss: 1.6319, Perplexity:  5.11\n",
            "Epoch [101/200], Loss: 1.6236, Perplexity:  5.07\n",
            "Epoch [102/200], Loss: 1.6205, Perplexity:  5.06\n",
            "Epoch [103/200], Loss: 1.6127, Perplexity:  5.02\n",
            "Epoch [104/200], Loss: 1.6092, Perplexity:  5.00\n",
            "Epoch [105/200], Loss: 1.6066, Perplexity:  4.99\n",
            "Epoch [106/200], Loss: 1.6041, Perplexity:  4.97\n",
            "Epoch [107/200], Loss: 1.6021, Perplexity:  4.96\n",
            "Epoch [108/200], Loss: 1.5962, Perplexity:  4.93\n",
            "Epoch [109/200], Loss: 1.5886, Perplexity:  4.90\n",
            "Epoch [110/200], Loss: 1.5878, Perplexity:  4.89\n",
            "Epoch [111/200], Loss: 1.5867, Perplexity:  4.89\n",
            "Epoch [112/200], Loss: 1.5787, Perplexity:  4.85\n",
            "Epoch [113/200], Loss: 1.5739, Perplexity:  4.83\n",
            "Epoch [114/200], Loss: 1.5709, Perplexity:  4.81\n",
            "Epoch [115/200], Loss: 1.5689, Perplexity:  4.80\n",
            "Epoch [116/200], Loss: 1.5656, Perplexity:  4.79\n",
            "Epoch [117/200], Loss: 1.5596, Perplexity:  4.76\n",
            "Epoch [118/200], Loss: 1.5588, Perplexity:  4.75\n",
            "Epoch [119/200], Loss: 1.5547, Perplexity:  4.73\n",
            "Epoch [120/200], Loss: 1.5508, Perplexity:  4.72\n",
            "Epoch [121/200], Loss: 1.5494, Perplexity:  4.71\n",
            "Epoch [122/200], Loss: 1.5446, Perplexity:  4.69\n",
            "Epoch [123/200], Loss: 1.5425, Perplexity:  4.68\n",
            "Epoch [124/200], Loss: 1.5393, Perplexity:  4.66\n",
            "Epoch [125/200], Loss: 1.5327, Perplexity:  4.63\n",
            "Epoch [126/200], Loss: 1.5311, Perplexity:  4.62\n",
            "Epoch [127/200], Loss: 1.5278, Perplexity:  4.61\n",
            "Epoch [128/200], Loss: 1.5258, Perplexity:  4.60\n",
            "Epoch [129/200], Loss: 1.5244, Perplexity:  4.59\n",
            "Epoch [130/200], Loss: 1.5210, Perplexity:  4.58\n",
            "Epoch [131/200], Loss: 1.5151, Perplexity:  4.55\n",
            "Epoch [132/200], Loss: 1.5137, Perplexity:  4.54\n",
            "Epoch [133/200], Loss: 1.5097, Perplexity:  4.53\n",
            "Epoch [134/200], Loss: 1.5108, Perplexity:  4.53\n",
            "Epoch [135/200], Loss: 1.5081, Perplexity:  4.52\n",
            "Epoch [136/200], Loss: 1.5023, Perplexity:  4.49\n",
            "Epoch [137/200], Loss: 1.4953, Perplexity:  4.46\n",
            "Epoch [138/200], Loss: 1.4966, Perplexity:  4.47\n",
            "Epoch [139/200], Loss: 1.4998, Perplexity:  4.48\n",
            "Epoch [140/200], Loss: 1.4927, Perplexity:  4.45\n",
            "Epoch [141/200], Loss: 1.4863, Perplexity:  4.42\n",
            "Epoch [142/200], Loss: 1.4860, Perplexity:  4.42\n",
            "Epoch [143/200], Loss: 1.4833, Perplexity:  4.41\n",
            "Epoch [144/200], Loss: 1.4822, Perplexity:  4.40\n",
            "Epoch [145/200], Loss: 1.4799, Perplexity:  4.39\n",
            "Epoch [146/200], Loss: 1.4788, Perplexity:  4.39\n",
            "Epoch [147/200], Loss: 1.4725, Perplexity:  4.36\n",
            "Epoch [148/200], Loss: 1.4725, Perplexity:  4.36\n",
            "Epoch [149/200], Loss: 1.4687, Perplexity:  4.34\n",
            "Epoch [150/200], Loss: 1.4662, Perplexity:  4.33\n",
            "Epoch [151/200], Loss: 1.4661, Perplexity:  4.33\n",
            "Epoch [152/200], Loss: 1.4624, Perplexity:  4.32\n",
            "Epoch [153/200], Loss: 1.4628, Perplexity:  4.32\n",
            "Epoch [154/200], Loss: 1.4611, Perplexity:  4.31\n",
            "Epoch [155/200], Loss: 1.4584, Perplexity:  4.30\n",
            "Epoch [156/200], Loss: 1.4516, Perplexity:  4.27\n",
            "Epoch [157/200], Loss: 1.4532, Perplexity:  4.28\n",
            "Epoch [158/200], Loss: 1.4525, Perplexity:  4.27\n",
            "Epoch [159/200], Loss: 1.4529, Perplexity:  4.28\n",
            "Epoch [160/200], Loss: 1.4469, Perplexity:  4.25\n",
            "Epoch [161/200], Loss: 1.4465, Perplexity:  4.25\n",
            "Epoch [162/200], Loss: 1.4443, Perplexity:  4.24\n",
            "Epoch [163/200], Loss: 1.4450, Perplexity:  4.24\n",
            "Epoch [164/200], Loss: 1.4392, Perplexity:  4.22\n",
            "Epoch [165/200], Loss: 1.4333, Perplexity:  4.19\n",
            "Epoch [166/200], Loss: 1.4339, Perplexity:  4.19\n",
            "Epoch [167/200], Loss: 1.4321, Perplexity:  4.19\n",
            "Epoch [168/200], Loss: 1.4300, Perplexity:  4.18\n",
            "Epoch [169/200], Loss: 1.4279, Perplexity:  4.17\n",
            "Epoch [170/200], Loss: 1.4274, Perplexity:  4.17\n",
            "Epoch [171/200], Loss: 1.4206, Perplexity:  4.14\n",
            "Epoch [172/200], Loss: 1.4233, Perplexity:  4.15\n",
            "Epoch [173/200], Loss: 1.4211, Perplexity:  4.14\n",
            "Epoch [174/200], Loss: 1.4183, Perplexity:  4.13\n",
            "Epoch [175/200], Loss: 1.4192, Perplexity:  4.13\n",
            "Epoch [176/200], Loss: 1.4168, Perplexity:  4.12\n",
            "Epoch [177/200], Loss: 1.4176, Perplexity:  4.13\n",
            "Epoch [178/200], Loss: 1.4123, Perplexity:  4.11\n",
            "Epoch [179/200], Loss: 1.4160, Perplexity:  4.12\n",
            "Epoch [180/200], Loss: 1.4117, Perplexity:  4.10\n",
            "Epoch [181/200], Loss: 1.4062, Perplexity:  4.08\n",
            "Epoch [182/200], Loss: 1.4086, Perplexity:  4.09\n",
            "Epoch [183/200], Loss: 1.4086, Perplexity:  4.09\n",
            "Epoch [184/200], Loss: 1.4054, Perplexity:  4.08\n",
            "Epoch [185/200], Loss: 1.4049, Perplexity:  4.07\n",
            "Epoch [186/200], Loss: 1.3997, Perplexity:  4.05\n",
            "Epoch [187/200], Loss: 1.3984, Perplexity:  4.05\n",
            "Epoch [188/200], Loss: 1.3978, Perplexity:  4.05\n",
            "Epoch [189/200], Loss: 1.3974, Perplexity:  4.04\n",
            "Epoch [190/200], Loss: 1.3973, Perplexity:  4.04\n",
            "Epoch [191/200], Loss: 1.3923, Perplexity:  4.02\n",
            "Epoch [192/200], Loss: 1.3909, Perplexity:  4.02\n",
            "Epoch [193/200], Loss: 1.3881, Perplexity:  4.01\n",
            "Epoch [194/200], Loss: 1.3909, Perplexity:  4.02\n",
            "Epoch [195/200], Loss: 1.3894, Perplexity:  4.01\n",
            "Epoch [196/200], Loss: 1.3843, Perplexity:  3.99\n",
            "Epoch [197/200], Loss: 1.3873, Perplexity:  4.00\n",
            "Epoch [198/200], Loss: 1.3852, Perplexity:  4.00\n",
            "Epoch [199/200], Loss: 1.3840, Perplexity:  3.99\n",
            "Epoch [200/200], Loss: 1.3812, Perplexity:  3.98\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 200\n",
        "train_loss_list = []\n",
        "\n",
        "# Truncated backpropagation\n",
        "# 逆伝播を途中で打ち切る\n",
        "def detach(states):\n",
        "    return [state.detach() for state in states] \n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0\n",
        "    # 初期隠れ状態とセル状態を設定する\n",
        "    states = (torch.zeros(1, batch_size, hidden_size).to(device),\n",
        "              torch.zeros(1, batch_size, hidden_size).to(device))\n",
        "    #train\n",
        "    net.train()\n",
        "    for i, batch in enumerate(train_iter):\n",
        "        text = batch.text.to(device)\n",
        "        labels = batch.target.to(device)\n",
        "        #LSTMの形状に合わせて入力もバッチを先にする。\n",
        "        text = text.permute(1, 0)\n",
        "        labels = labels.permute(1, 0)\n",
        "        \n",
        "        optim.zero_grad()\n",
        "        states = detach(states)\n",
        "        outputs, states = net(text, states)\n",
        "        loss = criterion(outputs, labels.reshape(-1))\n",
        "        train_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "    avg_train_loss = train_loss / len(train_iter)\n",
        "    print ('Epoch [{}/{}], Loss: {loss:.4f}, Perplexity: {perp:5.2f}' \n",
        "           .format(epoch+1, num_epochs, i+1, loss=avg_train_loss, perp=np.exp(avg_train_loss)))\n",
        "    train_loss_list.append(avg_train_loss)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oMumoWrTQp-O"
      },
      "source": [
        "## 生成"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        },
        "colab_type": "code",
        "id": "LN4FCjmpdfsp",
        "outputId": "67aaa8a6-5ef3-4412-aec4-d68cd2ad8359"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "drastically bottom watched rapidly trim visit sharing preferred cars there are also beginning to provide <unk> memory chips \n",
            "tandy corp. said it plans to sell its baby ruth <unk> and <unk> candy businesses to nestle foods corp. 's parent company \n",
            "<unk> is <unk> by montedison s.p a. of milan italy \n",
            "<unk> is subject to approval by former michael evans \n",
            "the offers though the accord is necessary to replace it completed its <unk> power \n",
            "the <unk> n.j. power producer is the result of a <unk> slowdown in the fields in the growing number of <unk> oil for coffee shares said \n",
            "as a result prices would have been <unk> and it could be <unk> down if a recession is looming challenge only one analyst here say \n",
            "perhaps the greatest share of the current comes is a result of the effect of the high price \n",
            "the order causes <unk> \n",
            "jack <unk> the consequences are being <unk> away \n",
            "<unk> <unk> \n",
            "associated with <unk> \n",
            "watch for something like <unk> a spokesman for the <unk> n.j. maker of <unk> <unk> <unk> entertainment inc. of <unk> corp. a joint venture with its approximately n billion midland nuclear plant \n",
            "the health-care company has n employees to get a n n stake in <unk> to n shares or n n effective through the acquisition launched by the company 's sale of n shares \n",
            "the company has suspended its plans to build a golden share buy-back program \n",
            "sci tv 's cineplex applied it is controlled by <unk> tenn. entrepreneur george <unk> \n",
            "a company spokesman declined to comment \n",
            "the company returned to usx 's false financial statement mr. <unk> said he described the primary majority in sci steel \n",
            "in return for this story the <unk> company makes certain government-owned entities \n",
            "advertising to be sold off showtime networks such as <unk> lines service under the <unk> of the network \n",
            "installation of satellite advanced n to n seats and <unk> advertising on the company 's class a class \n",
            "<unk> said it had a partial settlement with phone network improvements in the distribution \n",
            "at the same time he received $ n million in air france four <unk> projects according to a person 's image \n",
            "an <unk> mark recognition 's first casino company called <unk> <unk> and other <unk> <unk> film into <unk> <unk> \n",
            "in n the fda <unk> the <unk> to the cosmetics citizens which then gets mainly by the powerful computer service which puts the company to be an embarrassment to the art collection \n",
            "it 's a sale of <unk> to replace the <unk> <unk> <unk> book a <unk> <unk> <unk> show with <unk> visitors \n",
            "this is a <unk> for a <unk> <unk> to <unk> himself with a bit like something to do with something else \n",
            "and you can lose every job about <unk> i 'd rather be available for you to stay until a detroit \n",
            "this is a <unk> <unk> apple <unk> called <unk> 's <unk> to look at the <unk> of this wealthy ocean ocean ocean drilling producing the <unk> of modern progress \n",
            "along the way it into a <unk> patch of <unk> sheets or that <unk> <unk> <unk> and <unk> the <unk> <unk> associated with <unk> <unk> with <unk> <unk> \n",
            "while seita does n't exist on other crops \n",
            "it follows the same period to n n in n with the same as n low but eventually above n n would be about $ n trillion \n",
            "but it is n't yet clear whether the government would review and eastern will be necessary such objectives note will prompt anyone to know if the code for the clinical trial of the drug \n",
            "the department also said it is <unk> for a test and a <unk> system \n",
            "taxes tax or loan guarantees could be no greater than the n n of urban households that far fewer than n n of calls for abortions for disease woman champion of minutes the english off \n",
            "after <unk> <unk> a gray <unk> of <unk> <unk> <unk> <unk> from <unk> <unk> to a <unk> woman and <unk> her responsibility \n",
            "i feel god for all she says <unk> and <unk> at the <unk> <unk> in n \n",
            "in fact ms. volokh was an extreme emotional reaction to her and sometimes if you have to <unk> out you wish your <unk> <unk> to be no casual \n",
            "the <unk> of the <unk> of the <unk> 's building was seen as the <unk> of <unk> <unk> a <unk> <unk> was <unk> <unk> and <unk> and <unk> <unk> built into a <unk> <unk> <unk> with <unk> <unk> <unk> his rothschild kids ' <unk> \n",
            "he knows the woman and <unk> his <unk> as many families no longer pass \n",
            "it was a <unk> \n",
            "it 's a <unk> and a danger of <unk> <unk> or <unk> \n",
            "the <unk> of the <unk> ms. bartlett had to take her infection die \n",
            "i was <unk> for you \n",
            "mrs. ward says she noticed some woman <unk> up students or <unk> or <unk> \n",
            "what a woman ought to do something about mrs. yeargin she could test case \n",
            "a republican is to cool well the <unk> can be <unk> from reality and making money in a <unk> that he picked up \n",
            "<unk> e. <unk> are now relieved of the <unk> 's powers \n",
            "but the two-day trip this week is <unk> \n",
            "the house energy congress are <unk> the police department to <unk> boards of caution is <unk> \n",
            "britain 's <unk> raises the commitment of panamanian <unk> \n",
            "<unk> morishita 's silver industry publications long ago \n",
            "but he estimated that at the naval banks will take a more difficult battle for the sunday near district to oct. n by a house-senate conference \n",
            "denied rare senators make goals <unk> \n",
            "and yet feel good will \n"
          ]
        }
      ],
      "source": [
        "num_samples = 1000     # サンプリングされる単語の数\n",
        "# モデルをテストする\n",
        "net.eval()\n",
        "with torch.no_grad():\n",
        "    text = ''\n",
        "    # 初期隠れ状態とセル状態を設定する\n",
        "    states = (torch.zeros(1, 1, hidden_size).to(device),\n",
        "              torch.zeros(1, 1, hidden_size).to(device))\n",
        "\n",
        "    # ランダムに1単語のIDを選択\n",
        "    input = torch.multinomial(torch.ones(vocab_size), num_samples=1).unsqueeze(1).to(device)\n",
        "#     print(\"input word\", TEXT.vocab.itos[input])\n",
        "    \n",
        "    for i in range(num_samples):\n",
        "#         print(\"input word\", TEXT.vocab.itos[input])\n",
        "        \n",
        "        output, states = net(input, states)\n",
        "        word_id = output.max(1)[1].item()\n",
        "        # 次のタイムステップのために単語IDを入力\n",
        "        input.fill_(word_id)\n",
        "        # 単語IDから文字を取得\n",
        "        word = TEXT.vocab.itos[word_id]\n",
        "        # textに書き込む\n",
        "        word = '\\n' if word == '<eos>' else word + ' '\n",
        "        text += word\n",
        "\n",
        "    # textを表示\n",
        "    print(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "2AfCqGtrhs8g"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "section5_4.ipynb",
      "provenance": [],
      "version": "0.3.2"
    },
    "kernelspec": {
      "display_name": "pytorch_handbook-cxE4JzpW",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13 (default, Feb 12 2023, 23:20:22) \n[GCC 7.5.0]"
    },
    "vscode": {
      "interpreter": {
        "hash": "1a1627ff8c3f0278be0285d4bfdc18dde91f8374edbf4a554c43de70be81795e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
